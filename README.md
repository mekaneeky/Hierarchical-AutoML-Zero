# Hivetrain AutoML Subnet

Welcome to the Hivetrains Incentivized AutoML Loss Subnet, a collaborative platform dedicated to revolutionizing deep learning by automating the discovery of  improved and novel neural network components such loss functions, activation functions and potentially new algorithms that surpass the current state-of-the art. Inspired by the AutoML Zero paper, we leverage genetic programming to progressively develop increasingly complex mathematical functions using evolutionary and gradient based optimization.

## Current Focus

Currently running on Bittensor netuid 47 (100 testnet), we're starting with a loss function search where miners are incentivesed to find better loss functions for a neural network. This is just the beginning of our journey into distributed AutoML. 
### Why This Is Needed
Deep learning models have achieved remarkable success across various domains, from computer vision and natural language processing to reinforcement learning and beyond. However, these models often rely on traditional loss functions like Cross-Entropy or Mean Squared Error, which may not fully capture the complexities of modern datasets and tasks.

### Challenges in Deep Learning:

**Complexity of Models**: Advanced architectures like Transformers, GANs, and deep convolutional networks introduce intricate behaviors that standard loss functions might not adequately address.

**Diverse Data Modalities**: With the rise of multimodal data (e.g., combining text, images, and audio), there's a need for loss functions that can handle heterogeneous data effectively.

**Optimization Difficulties**: Deep networks are prone to issues like vanishing/exploding gradients and local minima, which can be mitigated with better-designed loss functions.

### Benefits and Contribution to the AI Landscape
**Enhanced Model Performance**: Discovering loss functions specifically optimized for deep learning architectures can lead to significant improvements in accuracy, convergence speed, and generalization.

**Advancements in AI Capabilities**: Novel loss functions can enable models to learn more abstract representations, handle complex tasks, and exhibit emergent behaviors. 

**Efficiency in Training**: Optimized loss functions can reduce training times and computational resources by facilitating more effective optimization.
Facilitating Cutting-Edge Research: By providing a platform for experimenting with loss functions in deep learning, we support researchers in exploring uncharted territories of AI. 

**Community-Driven Innovation**: Incentivizing loss function search attracts a diverse group of experts/researcher, fostering an environment where groundbreaking ideas can emerge. 


## Participation

### As a Miner

You have two main approaches as a miner:

1. **Rely on Brains:**
   - Develop new functions in the target optimization area and write algorithms in our genetic format.
   - Create better optimization approaches than our default offerings.
   - Design more efficient miners to maximize your available compute resources.

2. **Rely on Compute:**
   - If you have substantial computational resources:
     - Run an independent miner.
     - Join a mining pool (Work in Progress).
     - Run a miner on the mining pool (note: rewards will be shared with other miners).

### As a Validator

We welcome validators and are committed to supporting you. We can assist with setup, automation, cost-reduction, and other measures to reduce friction. Please note: Do not copy weights.

## FAQs

**Q: Is there research backing your claims?**  
A: Yes, our work is inspired by and based on several research papers:
- [AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](https://arxiv.org/abs/2003.03384)
- [Lion: Adversarial Distillation of Closed-Source Large Language Model](https://arxiv.org/abs/2302.06675)
- For more AutoML research areas, refer to the [AutoML Conference 2024](https://2024.automl.cc/)

**Q: Are you done with distributed training?**  
A: Distributed training is a technically complex problem. We're still developing our solution, and our candidate approach is not yet ready for release.

## Getting Started

For detailed instructions on setting up and running miners and validators, please refer to our [Miner and Validator Tutorial](docs/tutorial.md).

## Community and Support

Join our community channels for discussions, support, and updates:
- [Discord](https://discord.com/channels/799672011265015819/1174839377659183174)

---

We're excited to have you join us on this journey of distributed AutoML. Let's build the hive.
